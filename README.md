# ğŸ“„ Document Search Assistant (RAG-based)

A **Retrieval-Augmented Generation (RAG)** system built from scratch using **HuggingFace models** and **Pinecone** to answer questions grounded in real documents (Google & Microsoft 10-K 2024 reports).

This project was intentionally built **without LangChain abstractions** at first, to deeply understand how each component works internally.

---

## ğŸš€ Features (Up to Phase 5)

* ğŸ“‚ PDF ingestion (static PDFs)
* âœ‚ï¸ Intelligent text chunking
* ğŸ”¢ HuggingFace embeddings (Sentence Transformers)
* â˜ï¸ Cloud vector storage using Pinecone
* ğŸ” Semantic search (vector similarity)
* ğŸ¤– Answer generation using Mistral (HuggingFace)
* ğŸ§  Fully grounded answers (no hallucination)
* ğŸ§± Modular, OOP-based architecture

---

## ğŸ§  Architecture Overview

```
PDFs
 â†’ Loader
 â†’ Chunker
 â†’ Embeddings
 â†’ Pinecone Vector Store
 â†’ Retriever
 â†’ LLM (Mistral)
 â†’ Answer
```

Each responsibility is isolated into its own module for clarity and scalability.

---

## ğŸ“ Project Structure

```
document-search-assistant/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/                  # Google & Microsoft 10-K PDFs (not committed)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loaders/
â”‚   â”‚   â””â”€â”€ pdf_loader.py      # PDF text extraction (PyMuPDF)
â”‚   â”‚
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â””â”€â”€ chunker.py         # Text chunking logic
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ hf_embeddings.py   # HuggingFace embedding model
â”‚   â”‚
â”‚   â”œâ”€â”€ vectorstore/
â”‚   â”‚   â””â”€â”€ pinecone_store.py  # Pinecone index, upsert & query
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ answer_generator.py # Mistral-based answer generation
â”‚
â”œâ”€â”€ main.py                    # Orchestrates the full RAG pipeline
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                       # API keys (not committed)
â””â”€â”€ README.md
```

---

## âš™ï¸ Tech Stack

* **Language**: Python 3.10+
* **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2`
* **LLM**: `mistralai/Mistral-7B-Instruct-v0.2`
* **Vector DB**: Pinecone (Serverless)
* **PDF Parsing**: PyMuPDF (`fitz`)
* **Inference**: HuggingFace Inference API

---

## ğŸ”‘ Environment Variables

Create a `.env` file:

```env
HUGGINGFACEHUB_API_TOKEN=hf_xxxxxxxxxxxxx
PINECONE_API_KEY=pcsk_xxxxxxxxxxxxx
PINECONE_ENV=us-east-1
PINECONE_INDEX_NAME=document-search-poc
```

---

## â–¶ï¸ How to Run (Phase 5)

```bash
# create & activate venv
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# install dependencies
pip install -r requirements.txt

# run the pipeline
python main.py
```

---

## ğŸ§ª What Happens When You Run It

1. PDFs are loaded from `data/pdfs`
2. Text is chunked into ~500-800 character segments
3. Each chunk is converted into a 384-dim vector
4. Vectors are stored in Pinecone (batched upserts)
5. User query is embedded
6. Pinecone retrieves top-K relevant chunks
7. Retrieved chunks are passed as **context**
8. Mistral generates a grounded answer

---

## ğŸ§  Key Concepts Learned (So Far)

* How RAG works internally (no black boxes)
* Why batching is required for vector DBs
* Difference between embeddings and vectors
* How `__init__` and object state matter in ML systems
* Why context formatting (`"\n\n".join(contexts)`) matters
* How to debug real Pinecone & HuggingFace API errors
* Difference between `text_generation` vs `chat_completion`

---

## ğŸ§¾ Example Query

```text
What is the policy regarding incentive compensation recovery?
```

The answer is generated **only** from Google & Microsoft 10-K filings, not from the modelâ€™s training data.

---

## ğŸ”® Future Improvements

### ğŸš§ Phase 6 - API & UI Layer

* Convert pipeline into **FastAPI backend**
* Add `/query` endpoint
* Add request/response schemas
* Build **Streamlit UI** for interaction
* Support live user queries
* Improve error handling & logging

---

### ğŸš§ Phase 7 - Production Readiness

* Upload PDFs dynamically (user uploads)
* Add document metadata:

  * company name
  * year
  * source
* Add **citations per answer**
* Introduce `src/qa/` RAG orchestration layer
* Dockerize the application
* CI/CD with GitHub Actions
* Optional: migrate to LangChain or LlamaIndex (with understanding)

---

## ğŸ¤ Interview-Ready Summary

> â€œI built a full RAG system using HuggingFace embeddings, Pinecone for semantic retrieval, and a Mistral-based LLM for grounded answer generation. I implemented the pipeline manually to deeply understand each component before adding abstractions.â€

---

## âœ… Status

* âœ” Phase 1-5: **Completed**
* ğŸ”œ Phase 6-7: **Planned**